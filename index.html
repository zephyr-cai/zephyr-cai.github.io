<!DOCTYPE html>
<html lang="en">
<head>
	<meta name="generator" content="Hugo 0.42.1" />
	<meta name="robots" content="noodp" />
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	
	<title>Xufeng Cai</title>
	
	<meta name="description" content="Xufeng Cai's homepage">
	<meta name="image" content="">
	<meta name="author" content="Xufeng Cai">
    	<meta name="keywords" content="Xufeng Cai, SJTU, UW-Madison, Optimization, Deep Learning (DL), Machine Learning (ML), Mathematics">
	
	<meta name="og:title" content="Xufeng Cai">
	<meta name="og:description" content="Xufeng Cai's homepage">
	
	<meta name="og:url" content="https://xufengcai.com/">
	<meta name="og:site_name" content="">
	<meta name="og:type" content="article">
	
	<meta name="article:tag" content="">
	<link rel="stylesheet" type="text/css" href="./style.css">
	<link rel="icon" type="image/ico" href="/icon.ico">
    	<link rel="shortcut-icon" type="image/ico" href="/icon.ico">

	
	
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-72495497-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	<script defer src="/static/fontawesome/fontawesome-all.js"></script>

<script>
	function toggle_menu() {
		var x = document.getElementById("menu_bar");
		if (x.style.display === "none") {
			x.style.display = "inline";
		} else {
			x.style.display = "none";
		}
	}
</script>

<script> 
	
	// Function to toggle the plus menu into minus 
	function rev_menu_bar_toggle(x) { 
		x.classList.toggle("af_menu_icon"); 
	} 
</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>

<body>

<header>
	<div style="text-align: bottom">
			<!-- Bar icon for navigation -->
				<a href="javascript:void(0);" class="icon" onclick="toggle_menu()" style="color: #333;"> 
				<span onclick="rev_menu_bar_toggle(this)" class="menu_icon"></span>
				</a>
				&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="/" class="namelogo">Xufeng Cai</a>
				<div id="menu_bar", style="display: none; float: right;"> 
				<menu>
					<a href="/profiles/XufengCai_CV_041524.pdf" style="color:#333;">CV</a>
					<!--
					&nbsp;&nbsp;&nbsp;
					<a href="/blog/" style="color:#333;">Blog</a>
					&nbsp;&nbsp;&nbsp;
					<a href="/reading-list/" style="color:#333;">Reading List</a>
					-->
				</menu> 
				</div>	 
	</div>
	<!--
		<div style="text-align: bottom">
		<a href="https://xufengcai.com/" style="float: left;" class="namelogo">Xufeng Cai</a>
	
	<a href="https://xufengcai.com/Free-Time-List/" style="color:#777; float: right">Free Time List</a>
	<a href="https://xufengcai.com/Blog/" style="color:#777; float: right">Blog&nbsp;&nbsp;&nbsp;</a>
	<a href="https://xufengcai.com/Resources/" style="color:#777; float: right">Resources&nbsp;&nbsp;&nbsp;</a>
	<a href="https://xufengcai.com/profiles/cv.pdf" style="color:#777; float: right">CV&nbsp;&nbsp;&nbsp;</a>
	
	</div>
	-->
</header>



<div class="content">
  <h1></h1>
  <aside></aside>
  <p>

<p><img class="profile-picture" src="Xufeng_Gollum.jpg"></p>

<p> I am currently a fifth year PhD candidate at the <a href="https://www.cs.wisc.edu/">Department of Computer Sciences, UW-Madison</a>. I am fortunate to be advised by Prof. <a href="http://www.jelena-diakonikolas.com/">Jelena Diakonikolas</a>.
	Earlier, I received my BS in Math from <a href="https://en.zhiyuan.sjtu.edu.cn/">Zhiyuan Honors College</a> at Shanghai Jiao Tong University.
	</p>

<p>My research interests are primarily focused on <b>optimization</b> and <b>machine learning</b>. </p>

<p>Please reach out via xcai74 [at] wisc [dot] edu.</p>

<p><br></p>

<script>
function absSUM() {
	var x = document.getElementById("abs-summit");
	if (x.style.display === "none") {
		x.style.display = "block";
	} else {
		x.style.display = "none";
	}
}
</script>

<script>
function absICM() {
	var x = document.getElementById("abs-icm");
	if (x.style.display === "none") {
		x.style.display = "block";
	} else {
		x.style.display = "none";
	}
}
</script>

<script>
function absHAL() {
	var x = document.getElementById("abs-halpern");
	if (x.style.display === "none") {
		x.style.display = "block";
	} else {
		x.style.display = "none";
	}
}
</script>	

<script>
function absCCD() {
	var x = document.getElementById("abs-nonconvex-ccd");
	if (x.style.display === "none") {
		x.style.display = "block";
	} else {
		x.style.display = "none";
	}
}
</script>

<script>
function absShuffledSGD() {
	var x = document.getElementById("abs-shuffled-sgd");
	if (x.style.display === "none") {
		x.style.display = "block";
	} else {
		x.style.display = "none";
	}
}
</script>

<script>
function absFiniteHalpern() {
	var x = document.getElementById("abs-finite-Halpern");
	if (x.style.display === "none") {
		x.style.display = "block";
	} else {
		x.style.display = "none";
	}
}
</script>

<script>
function absCL() {
	var x = document.getElementById("abs-CL");
	if (x.style.display === "none") {
		x.style.display = "block";
	} else {
		x.style.display = "none";
	}
}
</script>

<script>
function absHyperZero() {
	var x = document.getElementById("abs-hyperzero");
	if (x.style.display === "none") {
		x.style.display = "block";
	} else {
		x.style.display = "none";
	}
}
</script>


<h4 id="publications" class='subtitle'>Recent Papers</h4><small>(reverse chronological, * denotes equal contribution)</small>

<p><b>Last Iterate Convergence of Incremental Methods as a Model of Forgetting.</b>
<br>
Xufeng Cai, <a href="http://www.jelena-diakonikolas.com/">Jelena Diakonikolas</a>.
<br>
In Proc. <a href="https://iclr.cc/">ICLR'25</a>, 2025.
<br>
<a id="abs-CL-button" onclick="absCL()" class="morecontent">abstract</a> / <a href="https://arxiv.org/abs/2403.06873" class="morecontent">arXiv</a></p>
<div id="abs-CL" style="display:none;">
<blockquote>Incremental gradient and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, 
	broadly studied in the literature. Yet, without strong convexity, their convergence guarantees have primarily been established for the ergodic (average) iterate. 
	We establish the first nonasymptotic convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, 
	in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings. Our oracle complexity bounds for the last iterate nearly match
	(i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods. 
	We further obtain generalizations of our results to weighted averaging of the iterates with increasing weights and for randomly permuted ordering of updates. 
	We study last iterate convergence of the incremental proximal method as a mathematical abstraction of forgetting in continual learning and prove a lower bound 
	that certifies that a large amount of regularization is crucial to mitigating catastrophic forgetting---one of the key considerations in continual learning.
	Our results generalize last iterate guarantees for incremental methods compared to state of the art, as such results were previously known only for overparameterized linear models, 
	which correspond to convex quadratic problems with infinitely many solutions.
</blockquote>
</div>

<p><b>HyperZero: A Customized End-to-End Auto-Tuning System for Recommendation with Hourly Feedback.</b>
<br>
Xufeng Cai*, <a href="https://www.linkedin.com/in/ziwei-guan/">Ziwei Guan*</a>, <a href="https://www.linkedin.com/in/lei-yuan-7b490b19/">Lei Yuan*</a>, <a href="https://www.linkedin.com/in/aliselmanaydin/">Ali Selman Aydin*</a>, 
<a href="https://www.linkedin.com/in/tengyu-xu-5826a8109/">Tengyu Xu</a>, <a href="https://www.linkedin.com/in/boyingliu/">Boying Liu</a>, <a href="https://www.linkedin.com/in/wenbo-ren-ab8a20126/?locale=en_US">Wenbo Ren</a>, 
<a href="https://www.linkedin.com/in/renkai-xiang-99056884/">Renkai Xiang</a>, <a href="https://www.linkedin.com/in/hesongyi/">Songyi He</a>, <a href="https://www.linkedin.com/in/hyang-aiml/">Haichuan Yang</a>, Serena Li, 
<a href="https://www.linkedin.com/in/mingze-gao-349a3273/">Mingze Gao</a>, <a href="https://www.linkedin.com/in/yue-weng-32918147/">Yue Weng</a>, <a href="https://www.linkedin.com/in/ji-liu-05343a28/">Ji Liu</a>.
<br>
In Proc. <a href="https://kdd2025.kdd.org/">KDD'25</a>, 2025.
<br>
<a id="abs-hyperzero-button" onclick="absHyperZero()" class="morecontent">abstract</a> / <a href="https://arxiv.org/abs/2501.18126" class="morecontent">arXiv</a></p>
<div id="abs-hyperzero" style="display:none;">
<blockquote>Modern recommendation systems can be broadly divided into two key stages: the ranking stage, where the system predicts the various user's engagements 
(e.g., the click through rate, the like rate, the follow rate, the watch time), and the value model stage, which aggregates these predictive scores through a function 
(e.g., a linear combination defined by a weight vector) to measure the value of each content by a single score number. 
Both stages play roughly equal importance in real industrial systems; how to optimize the model weights for the second stage still lack systematical study. 
This paper focuses on how to optimize the second stage through auto-tuning technology. Although general auto-tuning system/solution, both from established production practices and open-source solutions, 
can address this problem, they often require weeks or even months to identify a feasible solution. This prolonged tuning process is unacceptable in production environments of recommendation systems, 
as suboptimal value models can severely degrade user experience and negatively impact company revenue. An effective auto-tuning solution is required to identify a viable model within a matter of 2-3 days, 
rather than the extended timelines typically associated with existing approaches. 
In this paper, we introduce a practical auto-tuning system named HyperZero that addresses these time constraints while effectively solving the unique challenges inherent in modern recommendation systems. 
Moreover, this approach has the potential to be expanded to broader tuning tasks within recommendation systems. 
This system has been empirically validated, demonstrating significant improvements in key topline metrics on a large scale video platform (with hundred of millions DAU), 
including enhanced user experience indicators, and revenue growth, following its deployment across diverse settings.
</blockquote>
</div>

<p><b>Tighter Convergence Bounds for Shuffled SGD via Primal-Dual Perspective.</b>
<br>
Xufeng Cai*, <a href="https://ericlincc.com/">Cheuk Yin Lin*</a>, <a href="http://www.jelena-diakonikolas.com/">Jelena Diakonikolas</a>.
<br>
In Proc. <a href="https://neurips.cc/">NeurIPS'24</a>, 2024.
<br>
<a id="abs-shuffled-sgd-button" onclick="absShuffledSGD()" class="morecontent">abstract</a> / <a href="https://arxiv.org/abs/2306.12498" class="morecontent">arXiv</a></p>
<div id="abs-shuffled-sgd" style="display:none;">
<blockquote>Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern
machine learning. Contrary to the empirical practice of sampling from the datasets without replacement and
with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption
of sampling with replacement. It is only very recently that SGD with sampling without replacement – shuffled SGD – has been analyzed. For convex finite sum problems with n components and under the
\(L\)-smoothness assumption for each component function, there are matching upper and lower bounds, under
sufficiently small – \(\mathcal{O}(\frac{1}{nL})\) – step sizes. Yet those bounds appear too pessimistic – in fact, the predicted
performance is generally no better than for full gradient descent – and do not agree with the empirical
observations. In this work, to narrow the gap between the theory and practice of shuffled SGD, we sharpen
the focus from general finite sum problems to empirical risk minimization with linear predictors. This
allows us to take a primal-dual perspective and interpret shuffled SGD as a primal-dual method with cyclic
coordinate updates on the dual side. Leveraging this perspective, we prove a fine-grained complexity bound
that depends on the data matrix and is never worse than what is predicted by the existing bounds. Notably,
our bound can predict much faster convergence than the existing analyses – by a factor of the order of \(\sqrt{n}\)
in some cases. We empirically demonstrate that on common machine learning datasets our bound is indeed
much tighter. We further show how to extend our analysis to convex nonsmooth problems, with similar improvements.
</blockquote>
</div>

<p><b>Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions.</b>
<br>
Xufeng Cai*, <a href="https://ahmetalacaoglu.github.io/">Ahmet Alacaoglu</a>*, <a href="http://www.jelena-diakonikolas.com/">Jelena Diakonikolas</a>.
<br>
In Proc. <a href="https://iclr.cc/">ICLR'24</a>, 2024.
<br>
<a id="abs-finite-Halpern-button" onclick="absFiniteHalpern()" class="morecontent">abstract</a> / <a href="https://arxiv.org/abs/2310.02987" class="morecontent">arXiv</a></p>
<div id="abs-finite-Halpern" style="display:none;">
<blockquote>Machine learning approaches relying on such criteria as adversarial robustness or multi-agent settings have raised the need for solving game-theoretic equilibrium problems. 
Of particular relevance to these applications are methods targeting finite-sum structure, which generically arises in empirical variants of learning problems in these contexts. 
Further, methods with computable approximation errors are highly desirable, as they provide verifiable exit criteria. Motivated by these applications, 
we study finite-sum monotone inclusion problems, which model broad classes of equilibrium problems. 
Our main contributions are variants of the classical Halpern iteration that employ variance reduction to obtain improved complexity guarantees in which \(n\) component operators in the finite sum are ``on average'' 
either cocoercive or Lipschitz continuous and monotone, with parameter \(L\). The resulting oracle complexity of our methods, which provide guarantees for the last iterate and for a (computable) operator norm residual, 
is \(\widetilde{\mathcal{O}}( n + \sqrt{n}L\varepsilon^{-1})\), which improves upon existing methods by a factor up to \(\sqrt{n}\). 
This constitutes the first variance reduction-type result for general finite-sum monotone inclusions and for more specific problems such as convex-concave optimization when operator norm residual is the optimality measure. 
We further argue that, up to poly-logarithmic factors, this complexity is unimprovable in the monotone Lipschitz setting; i.e., the provided result is near-optimal.
</blockquote>
</div>

<p><b>Cyclic Block Coordinate Descent With Variance Reduction for Composite Nonconvex Optimization.</b>
<br>
Xufeng Cai, <a href="https://sites.google.com/view/chaobing-song/home">Chaobing Song</a>, <a href="https://pages.cs.wisc.edu/~swright/">Stephen J. Wright</a>, <a href="http://www.jelena-diakonikolas.com/">Jelena Diakonikolas</a>.
<br>
In Proc. <a href="https://icml.cc/">ICML'23</a>, 2023.
<br>
<a id="abs-nonconvex-ccd-button" onclick="absCCD()" class="morecontent">abstract</a> / <a href="https://arxiv.org/abs/2212.05088" class="morecontent">arXiv</a></p>
<div id="abs-nonconvex-ccd" style="display:none;">
<blockquote>Nonconvex optimization is central in solving many machine learning problems, in which block-wise structure is
commonly encountered. In this work, we propose cyclic block coordinate methods for nonconvex optimization problems
with non-asymptotic gradient norm guarantees. Our convergence analysis is based on a gradient Lipschitz condition
with respect to a Mahalanobis norm, inspired by a recent progress on cyclic block coordinate methods. In deterministic
settings, our convergence guarantee matches the guarantee of (full-gradient) gradient descent, but with the gradient
Lipschitz constant being defined w.r.t. a Mahalanobis norm. In stochastic settings, we use recursive variance reduction to
decrease the per-iteration cost and match the arithmetic operation complexity of current optimal stochastic full-gradient
methods, with a unified analysis for both finite-sum and infinite-sum cases. We prove a faster linear convergence result
when a Polyak-Łojasiewicz (PŁ) condition holds. To our knowledge, this work is the first to provide non-asymptotic
convergence guarantees — variance-reduced or not — for a cyclic block coordinate method in general composite (smooth
+ nonsmooth) nonconvex settings. Our experimental results demonstrate the efficacy of the proposed cyclic scheme in
training deep neural nets.
</blockquote>
</div>

<p><b>Stochastic Halpern Iteration with Variance Reduction for Stochastic Monotone Inclusions.</b>
<br>
Xufeng Cai, <a href="https://sites.google.com/view/chaobing-song/home">Chaobing Song</a>, <a href="https://sites.google.com/view/cguzman/">Cristóbal Guzmán</a>, <a href="http://www.jelena-diakonikolas.com/">Jelena Diakonikolas</a>.
<br>
In Proc. <a href="https://neurips.cc/">NeurIPS'22</a>, 2022.
<br>
<a id="abs-halpern-button" onclick="absHAL()" class="morecontent">abstract</a> / <a href="https://arxiv.org/abs/2203.09436" class="morecontent">arXiv</a></p>
<div id="abs-halpern" style="display:none;">
<blockquote>We study stochastic monotone inclusion problems, which widely appear in machine learning applications, 
	including robust regression and adversarial learning. We propose novel variants of stochastic Halpern iteration with recursive variance reduction. 
	In the cocoercive---and more generally Lipschitz-monotone---setup, 
	our algorithm attains \(\epsilon\) norm of the operator with \(\mathcal{O}(\frac{1}{\epsilon^3})\) stochastic operator evaluations, 
	which significantly improves over state of the art \(\mathcal{O}(\frac{1}{\epsilon^4})\) stochastic operator evaluations 
	required for existing monotone inclusion solvers applied to the same problem classes. %is the first result of this kind. 
We further show how to couple one of the proposed variants of stochastic Halpern iteration 
	with a scheduled restart scheme to solve stochastic monotone inclusion problems with \({\mathcal{O}}(\frac{\log(1/\epsilon)}{\epsilon^2})\)
	stochastic operator evaluations under additional sharpness or strong monotonicity assumptions.
</blockquote>
</div>

	
<h4 id="talks" class='subtitle'>Talks</h4>
<p><b>Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions.</b>
<br>
<a href="https://meetings.informs.org/wordpress/seattle2024/">2024 INFORMS Annual Meeting (INFORMS)</a>, Seattle, WA, USA. 2024.
<br>
<p><b>Stochastic Halpern Iteration with Variance Reduction for Stochastic Monotone Inclusions.</b>
<br>
<a href="https://iccopt2022.lehigh.edu/">The seventh International Conference on Continuous Optimization (ICCOPT)</a>, Bethlehem, PA, USA. 2022.
<br>


<h4 id="Teaching" class="subtitle">Teaching</h4>
<p>
	<b>CS639</b>, Foundations of Data Science, UW-Madison, Spring 2024 (TA).</br>
	<b>CS639</b>, Foundations of Data Science, UW-Madison, Spring 2022 (TA).</br>
	<b>CS760</b>, Machine Learning, UW-Madison, Spring 2021 (TA).</br>
	<b>CS760</b>, Machine Learning, UW-Madison, Fall 2020 (TA).
</p>

<!--
<h4 id="contest" class='subtitle'>Contests</h4>
<p><b>AI Solution for In Vitro Diagnostics and Pharmaceutical Business</b>
<br>
<a href="https://www.facebook.com/events/shanghai-advanced-institute-of-finance-master-of-finance/2019-saif-mf-international-youth-leadership-finance-summit/326413831484071/">International Youth Leadership Finance Summit</a>, 
<a href="http://en.saif.sjtu.edu.cn/">SAIF</a>, 2019
<br>
<a id="abs-summit-button" onclick="absSUM()" class="morecontent">abstract</a> / <a href="./contest/2019IYLFS_slide.pdf" class="morecontent">slides</a></p>
<div id="abs-summit" style="display:none;">
<blockquote>We designed AI solutions for a real health company <i>Haiershi</i> to its in vitro diagnostics and pharmaceutical business. 
	We also presented a roadshow to the investors in order to seek equity financing in the summit.
</blockquote>
</div>

<p><b>Climate-based Fragility Measurement Model for Regions in the World</b>
<br>
<a href="https://www.comap.com/undergraduate/contests/index.html">The Interdisciplinary Contest in Modeling</a>, 
<a href="https://www.comap.com/">Comap</a>, 2018
<br>
<a id="abs-icm-button" onclick="absICM()" class="morecontent">abstract</a> / <a href="./contest/2018ICM.pdf" class="morecontent">report</a></p>
<div id="abs-icm" style="display:none;">
<blockquote>
		We designed the SPEC index to evaluate regional fragility concerning effects of climate change, and formulated the self-regulatory factor to model the tipping-point of regional stabilities.
		We futher collected and cleaned relevant data of 178 countries worldwide to test our model. Our SPEC score corresponded to the traditional Fragile State Index but stressed the effects of climate change.
		Eventually our team was designated as Outstanding Winner (top 0.16% in over 20,000 teams in the world).
</blockquote>
</div>
-->

<!--
<h2 id="explorations">Exploration</h2>
<p><strong>Adaptive Moving Mesh Method for Blow-up Equations with e<sup>μ</sup> Nonlinearity</strong>
<br>
Coursework for <i>Numerical Methods for Differential Equations</i>, Spring 2019
<br>
[<a href="./explorations/2019NumPDE.pdf">report</a>, <a href=https://github.com/zephyr-cai/Numerical-Moving-Mesh-Method>code</a>]</p>
-->

<h4 id="hobbies" class="subtitle">Miscellaneous</h4>
<p>In my free time, I enjoy reading, photography, and music (vinyls).</p>

</p>
</div>

<footer>
	<div style="text-align: top">
		<div style="float: left;">
		<p class="footer-subtitle" style="font-size: 90%; color: #333; padding: 0;">OFFICE </br>
		<span style="font-family: serif; line-height: 1.5; color: #373a3c; font-size: 90%;">Room 4376, Dept of Computer Sciences, UW-Madison.</span>
		</p>
		</div>
		<div style="float: right;">
		<p class="footer-subtitle" style="font-size: 90%; color: #333; padding: 0;">GET IN TOUCH </br>
		<a href="https://www.facebook.com/xufeng.cai.9" class="morecontent" style="font-size: 80%;">FACEBOOK</a> / 
		<a href="https://www.instagram.com/zephyrcai/" class="morecontent" style="font-size: 80%;">INSTAGRAM</a> / 
		<a href="https://cn.linkedin.com/in/xufeng-cai-1777751b4" class="morecontent" style="font-size: 80%;">LINKEDIN</a>
		</p>
		</div>
	</div>
</footer>
<div style="text-align: center;">
<p class="footer-claim">&copy; 2025 Xufeng Cai.</p>
</div>
</body>
</html>
